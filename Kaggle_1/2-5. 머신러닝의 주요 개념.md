# Part II. 머신러닝 문제해결

- 머신러닝 모델을 사용하는 캐글 경진대회에 익숙해지자!






# Chapter 5. 다시 살펴보는 머신러닝 주요 개념







### 목차

- 분류와 회귀
    - 회귀 평가지표
- 분류 평가지표
    - 오차 행렬
        - 정확도
        - 정밀도
        - 재현율
        - F1 점수
    - 로그 손실
    - ROC, AUC
- 데이터 인코딩
    - 레이블 인코딩
    - 원-핫 인코딩
- 피처 스케일링
    - min-max 정규화
    - 표준화
- 교차검증
    - K 폴드
    - 층화 K 폴드
- 주요 머신러닝 모델
    - 선형 회귀
    - 로지스틱 회귀
    - 결정 트리
    - 앙상블
    - 랜덤 포레스트
    - XGBoost
    - LightGBM
- 하이퍼파라미터 최적화
    - 그리드서치
    - 랜덤서치



## 5-1. 분류와 회귀

- 대부분의 캐글 경진대회는 분류나 회귀 문제를 다룬다.
- 특히 분류 문제가 많다.


### 사이킷런의 `metrics` 라이브러리를 활용해 회귀 평가지표 값 구하기

```python
import numpy as np
# MAE, MSE, MSLE, R2 임포트
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score

true = np.array([1, 2, 3, 2, 3, 5, 4, 6, 5, 6, 7, 8, 8]) # 실제값
preds = np.array([1, 1, 2, 2, 3, 4, 4, 5, 5, 7, 7, 6, 8]) # 예측값

MAE = mean_absolute_error(true, preds)
MSE = mean_squared_error(true, preds)
RMSE = np.sqrt(MSE)
MSLE = mean_squared_log_error(true, preds)
RMSLE = np.sqrt(mean_squared_log_error(true, preds))
R2 = r2_score(true, preds)

# 출력
print(f'MAE:\t {MAE:.4f}')
print(f'MSE:\t {MSE:.4f}')
print(f'RMSE:\t {RMSE:.4f}')
print(f'MSLE:\t {MSLE:.4f}')
print(f'RMSLE:\t {RMSLE:.4f}')
print(f'R2:\t {R2:.4f}')
```

```
MAE:	 0.5385
MSE:	 0.6923
RMSE:	 0.8321
MSLE:	 0.0296
RMSLE:	 0.1721
R2:	     0.8617
```


## 5-2. 분류 평가지표

- 분류 문제에서 사용하는 평가지표를 알아보자
    - 오차 행렬
    - 로그 손실
    - ROC 곡선
    - AUC

### 5-2-1. 오차 행렬 (Confusion matrix)

![img](./img/2-5-1.png)

- 참 양성, 거짓 양성, 거짓 음성, 참 음성은 각각 약어로 TP, FP, FN, TN으로 표시
- 오차 행렬을 활용한 주요 평가지표
    - 정확도(Accuracy)
        - $$\dfrac{TP + TN}{TP + FP + FN + TN}$$
    - 정밀도(Precision)
        - $$\dfrac{TP}{TP + FP}$$
        - 양성 예측의 정확도
        - 음성을 양성으로 잘못 판단하면 문제가 발생하는 경우
            - 양성: 검출하기 원하는 상태
    - 재현율(Recall)
        - $$\dfrac{TP}{TP + FN}$$
        - 실제 양성 값 중 양성으로 잘 예측한 값의 비율
        - 양성을 음성으로 잘못 판단하면 문제가 되는 경우
    - F1 점수(F1 Score)
        - $$F1 = \dfrac{2}{\frac{1}{precision} + \frac{1}{recall}} = 2\dfrac{precision \times recall}{precision + recall}$$
        - 정밀도와 재현율을 조합한 평가지표


### 5-2-2. 로그 손실(Logloss)
- 분류 문제에서 타깃값을 확률로 예측할 때 기본적으로 사용하는 평가지표
- 작을수록 좋음
$$logloss = - \dfrac{1}{N} \sum^N_{i=1} ( y_i\log(\hat y_i) + (1 - y_i)\log(1 - \hat y_i) )$$






### 5-2-3. ROC 곡선과 AUC

- **ROC(Receiver Operating Characteristic) 곡선**: 참 양성 비율(TPR)에 대한 거짓 양성 비율(FPR) 곡선
- **AUC(Area Under the Curve)**: ROC 곡선 아래 면적
- AUC는 기본적으로 예측값이 확률이 분류 문제에서 사용


> - 타깃값(이산값)으로 예측 시 분류 평가 지표: 정확도, 정밀도, 재현율, F1 점수
> - 타깃 확률로 예측 시 분류 평가지표: 로그 손실, AUC

![img](./img/2-5-2.png)


- TPR은 양성을 얼마나 정확히 예측 = 재현율 $$\dfrac{TP}{TP + FN}$$

- TNR은 음성을 얼마나 정확히 예측하는지 지표 = 특이도 (specificity) $$\dfrac{TN}{FP + TN}$$

- 거짓양성비율 (FPR) = 1 - TNR (민감도) $$FPR = \dfrac{FP}{FP+TN}$$




## 5-3. 데이터 인코딩

- 머신러닝 모델은 문제 데이터를 인식하지 못함
- 문자로 구성된 범주형 데이터는 숫자로 바꿔야 한다.

- **데이터 인코딩**: 범주형 데이터를 숫자 형태로 바꾸는 작업


### 5-3-1. 레이블 인코딩
- 범주형 데이터를 숫자로 일대일 매핑해주는 인코딩 방법
- 사이킷런의 `LabelEncoder`로 구현할 수 있다.
- 다음은 레이블 인코딩을 적용해 과일 문자열 데이터를 숫자형으로 변환하는 코드

```python
from sklearn.preprocessing import LabelEncoder # 레이블 인코더

fruits = ['사과', '블루베리', '바나나', '귤', '블루베리', '바나나', '바나나', '사과']

# 레이블 인코더 생성
label_encoder = LabelEncoder()
# 레이블 인코딩 적용
fruits_label_encoded = label_encoder.fit_transform(fruits)

print('레이블 인코딩 적용 후 데이터:', fruits_label_encoded)
```
```
레이블 인코딩 적용 후 데이터: [3 2 1 0 2 1 1 3]
```



- 단점: 명목형 데이터를 레이블 인코딩하면 모델 성능이 낮아짐
- 서로 가까운 숫자를 비슷한 데이터라고 판단하기 때문이다
- 이 문제는 원-핫 인코딩으로 해결할 수 있다. 


### 5-3-2. 원-핫 인코딩 (One-hot encoding)


- 원-핫 인코딩은 여러 값 중 하나(One)만 활성화(Hot)하는 인코딩이다. 
- 실행 절차
    1. 인코딩하려는 피처의 고윳값 개수
    2. 피처의 고윳값 개수만큼 열을 추가
    3. 각 고윳값에 해당하는 열에 1을 표시하고 나머지 열에는 0을 표시한다.




- 원-핫 인코딩은 레이블 인코딩의 문제(서로 가까운 숫자를 비슷한 데이터를 판단하는 문제)를 해결합니다.
- 단점: 열 개수가 지나치게 많아진다는 단점
    - 피처의 고윳값이 많으면 그만큼 열 개수와 메모리 사용량이 늘어나기 때문에 모델 훈련 속도가 느려질 우려

> 명목형 피처에 고윳값이 상당히 많을 땐 어떻게 해결해야 할까?
> 1. 비슷한 고윳값끼리 그룹화
>    - 그룹화하면 해당 명목형 피처의 고윳값 개수가 줄어드는 효과
> 2. 빈도가 낮은 고윳값을 '기타(etc)'로 처리하기
> 3. 다른 인코딩 적용하기
>    - 타깃인코딩, 프리퀀시 인코딩 등 그외 인코딩 기법
> - 한편 고윳값 개수가 많아도 데이터 크기가 그렇게 크지 않다면 그냥 원-핫 인코딩을 적용하기도 한다.
> - 데이터가 크지 않으니 열 개수가 늘어나도 모델 훈련 속도에 크게 영향을 주지 않기 때문

- 다음은 문자열 데이터를 원-핫 인코딩하는 코드
- 문자열 데이터에 바로 원-핫 인코딩을 적용할 순 없으니 레이블 인코딩으로 숫자형 데이터로 먼저 변환

```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

fruits = ['사과', '블루베리', '바나나', '귤', '블루베리', '바나나', '바나나', '사과']

# 레이블 인코더, 원-핫 인코더 생성
label_encoder = LabelEncoder()
onehot_encoder = OneHotEncoder()

# 레이블 인코딩 적용(문자 데이터 -> 숫자 데이터)
fruits_label_encoded = label_encoder.fit_transform(fruits)
# 원-핫 인코딩 적용
fruits_onehot_encoded = onehot_encoder.fit_transform(fruits_label_encoded.reshape(-1, 1))

print('원-핫 인코딩 적용 후 데이터:\n', fruits_onehot_encoded.toarray())
```

```
원-핫 인코딩 적용 후 데이터:
 [[0. 0. 0. 1.]
 [0. 0. 1. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [0. 0. 1. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 0. 1.]]
```


- 레이블 인코딩된 데이터는 1차원이라서 중간에 `reshape(-1,1)` 메서드를 이용해 2차원으로 바꿈

> `reshape()`용법
> - `reshape()` 메서드는 배열 형상을 바꿀 때 사용
> - 다음 1차원 배열인 `fruits_label_encoded`를 예로 `reshape()`용법을 알아봅시다.
> ```python
> fruits_label_encoded
> ```
> ```
> array([3, 2, 1, 0, 2, 1, 1, 3])
> ```
> - 이를 (4, 2) 형상의 행렬로 바꾸려면 다음과 같이 `reshape(4,2)`를 호출하면 됩니다.






```python
import pandas as pd

pd.get_dummies(fruits)
```
```
    귤	바나나	블루베리	사과
0	0	0	0	1
1	0	0	1	0
2	0	1	0	0
3	1	0	0	0
4	0	0	1	0
5	0	1	0	0
6	0	1	0	0
7	0	0	0	1
```