{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 4. 안전 운전자 예측 경진대회 성능 개선 I : LightGBM 모델\n\n- 베이스라인 모델과 같은 LightGBM을 그대로 사용하면서 피처 엔지니어링과 하이퍼파라미터 최적화 추가로 적용","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# 데이터 경로\ndata_path = '/kaggle/input/porto-seguro-safe-driver-prediction/'\n\ntrain = pd.read_csv(data_path + 'train.csv', index_col='id')\ntest = pd.read_csv(data_path + 'test.csv', index_col='id')\nsubmission = pd.read_csv(data_path + 'sample_submission.csv', index_col='id')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T10:38:24.127199Z","iopub.execute_input":"2023-04-03T10:38:24.127767Z","iopub.status.idle":"2023-04-03T10:38:36.118287Z","shell.execute_reply.started":"2023-04-03T10:38:24.127717Z","shell.execute_reply":"2023-04-03T10:38:36.117006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4-1. 피처 엔지니어링\n\n### 4-1-1. 데이터 합치기","metadata":{}},{"cell_type":"code","source":"all_data = pd.concat([train, test], ignore_index=True)\nall_data = all_data.drop('target', axis=1) # 타깃값 제거\n\nall_features = all_data.columns # 전체 피처","metadata":{"execution":{"iopub.status.busy":"2023-04-03T10:38:36.120673Z","iopub.execute_input":"2023-04-03T10:38:36.121136Z","iopub.status.idle":"2023-04-03T10:38:37.443722Z","shell.execute_reply.started":"2023-04-03T10:38:36.121088Z","shell.execute_reply":"2023-04-03T10:38:37.442444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-1-2. 명목적 피처 원-핫 인코딩","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# 명목형 피처\ncat_features = [feature for feature in all_features if 'cat' in feature] \n\n# 원-핫 인코딩 적용\nonehot_encoder = OneHotEncoder()\nencoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features]) ","metadata":{"execution":{"iopub.status.busy":"2023-04-03T10:38:37.445214Z","iopub.execute_input":"2023-04-03T10:38:37.445567Z","iopub.status.idle":"2023-04-03T10:38:39.871675Z","shell.execute_reply.started":"2023-04-03T10:38:37.445533Z","shell.execute_reply":"2023-04-03T10:38:39.870415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-1-3. 파생 피처 추가\n\n- EDA에서는 필요 없는 피처를 추리는 것 외에 특별한 피처 엔지니어링 요소를 찾아내지 못함.\n- 성능을 더 높이기 위해 시도 할 수 있는 몇 가지 방법 살피자\n\n\n### 1) **한 데이터가 가진 결측값 개수를 파생 피처로 만들자**","metadata":{}},{"cell_type":"code","source":"# '데이터 하나당 결측값 개수'를 파생 피처로 추가\nall_data['num_missing'] = (all_data==-1).sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T10:38:39.875258Z","iopub.execute_input":"2023-04-03T10:38:39.876191Z","iopub.status.idle":"2023-04-03T10:38:40.088704Z","shell.execute_reply.started":"2023-04-03T10:38:39.876139Z","shell.execute_reply":"2023-04-03T10:38:40.087384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 명목형 피처, calc 분류의 피처를 제외한 피처\nremaining_features = [feature for feature in all_features\n                      if ('cat' not in feature and 'calc' not in feature)] \n# num_missing을 remaining_features에 추가\nremaining_features.append('num_missing')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T10:38:40.090418Z","iopub.execute_input":"2023-04-03T10:38:40.090889Z","iopub.status.idle":"2023-04-03T10:38:40.097248Z","shell.execute_reply.started":"2023-04-03T10:38:40.090841Z","shell.execute_reply":"2023-04-03T10:38:40.096071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 이로서 `all_data`와 `remaining_features`에 새로운 파생 피처 `num_missing`이 추가됨\n\n### **2) 모든 `ind` 피처 값을 연결해서 새로운 피처를 만들자**\n\n- `mix_ind`\n- 뒤이어 추가할 '명목형 피처의 고윳값별 개수' 피처를 만들 때 사용하기 위한 임시 피처","metadata":{}},{"cell_type":"code","source":"# 분류가 ind인 피처\nind_features = [feature for feature in all_features if 'ind' in feature]\n\nis_first_feature = True\nfor ind_feature in ind_features:\n    if is_first_feature:\n        all_data['mix_ind'] = all_data[ind_feature].astype(str) + '_'\n        is_first_feature = False\n    else:\n        all_data['mix_ind'] += all_data[ind_feature].astype(str) + '_'","metadata":{"execution":{"iopub.status.busy":"2023-04-03T10:38:40.099120Z","iopub.execute_input":"2023-04-03T10:38:40.099720Z","iopub.status.idle":"2023-04-03T10:39:03.665377Z","shell.execute_reply.started":"2023-04-03T10:38:40.099669Z","shell.execute_reply":"2023-04-03T10:39:03.663951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data['mix_ind']","metadata":{"execution":{"iopub.status.busy":"2023-04-03T10:39:03.667186Z","iopub.execute_input":"2023-04-03T10:39:03.667541Z","iopub.status.idle":"2023-04-03T10:39:03.677584Z","shell.execute_reply.started":"2023-04-03T10:39:03.667507Z","shell.execute_reply":"2023-04-03T10:39:03.676047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **3) 명목형 피처의 고윳값별 개수를 새로운 피처로 추가**\n","metadata":{}},{"cell_type":"code","source":"cat_count_features = []\nfor feature in cat_features+['mix_ind']:\n    val_counts_dict = all_data[feature].value_counts().to_dict()\n    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x: \n                                                           val_counts_dict[x])\n    cat_count_features.append(f'{feature}_count')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T10:39:03.679054Z","iopub.execute_input":"2023-04-03T10:39:03.679417Z","iopub.status.idle":"2023-04-03T10:39:13.700051Z","shell.execute_reply.started":"2023-04-03T10:39:03.679383Z","shell.execute_reply":"2023-04-03T10:39:13.698907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_count_features","metadata":{"execution":{"iopub.status.busy":"2023-04-03T10:39:13.701763Z","iopub.execute_input":"2023-04-03T10:39:13.702136Z","iopub.status.idle":"2023-04-03T10:39:13.709330Z","shell.execute_reply.started":"2023-04-03T10:39:13.702105Z","shell.execute_reply":"2023-04-03T10:39:13.707693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 지금까지 만든 피처는 다음과 같다\n    - `encoded_cat_matrix`: 원-핫 인코딩된 명목형 피처\n    - `remaining_features`: 명목형 피처와 calc 분류의 피처를 제외한 피처들 (+ `num_missing`)\n    - `cat_count_features`: mix_ind 를 포함한 명목형 피러의 고윳값별 개수 파생 피처\n\n\n\n\n### 4-1-4. 필요 없는 피처 제거\n\n- 탐색적 데이터 분석에서 필요 없다고 판단한 피처는 제거한 다음\n- 지금까지 피처 엔지니어링한 모든 데이터를 합칠 것","metadata":{}},{"cell_type":"code","source":"from scipy import sparse\n# 필요 없는 피처들\ndrop_features = ['ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin', \n                 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14']\n\n# remaining_features, cat_count_features에서 drop_features를 제거한 데이터\nall_data_remaining = all_data[remaining_features+cat_count_features].drop(drop_features, axis=1)\n\n# 데이터 합치기\nall_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining),\n                               encoded_cat_matrix],\n                              format='csr')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T10:52:11.832838Z","iopub.execute_input":"2023-04-03T10:52:11.833999Z","iopub.status.idle":"2023-04-03T10:52:17.882553Z","shell.execute_reply.started":"2023-04-03T10:52:11.833954Z","shell.execute_reply":"2023-04-03T10:52:17.881072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 📖 지금까지의 피처 엔지니어링으로 수행한 일들을 정리하면 다음과 같다 \n\n- 명목형 피처에 원-핫 인코딩을 적용\n- 데이터 하나당 가지고 있는 결측값 개수를 새로운 피처로 만들었다.\n- 모든 ind 피처 값을 연결해서 새로운 명목형 피처를 만들었다.\n- 명목형 피처의 고윳값별 개수를 새로운 피처로 만들었다.\n- 필요 없는 피처를 제거 (drop_features와 calc 분류의 피처들)\n\n### 4-1-5. 데이터 나누기","metadata":{}},{"cell_type":"code","source":"num_train = len(train) # 훈련 데이터 개수\n\n# 훈련 데이터와 테스트 데이터 나누기\nX = all_data_sprs[:num_train]\nX_test = all_data_sprs[num_train:]\n\ny = train['target'].values","metadata":{"execution":{"iopub.status.busy":"2023-04-03T10:58:39.002795Z","iopub.execute_input":"2023-04-03T10:58:39.003486Z","iopub.status.idle":"2023-04-03T10:58:40.252842Z","shell.execute_reply.started":"2023-04-03T10:58:39.003422Z","shell.execute_reply":"2023-04-03T10:58:40.251448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef eval_gini(y_true, y_pred):\n    # 실제값과 예측값의 크기가 같은지 확인 (값이 다르면 오류 발생)\n    assert y_true.shape == y_pred.shape\n\n    n_samples = y_true.shape[0]                      # 데이터 개수\n    L_mid = np.linspace(1 / n_samples, 1, n_samples) # 대각선 값\n\n    # 1) 예측값에 대한 지니계수\n    pred_order = y_true[y_pred.argsort()] # y_pred 크기순으로 y_true 값 정렬\n    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n    G_pred = np.sum(L_mid - L_pred)       # 예측 값에 대한 지니계수\n\n    # 2) 예측이 완벽할 때 지니계수\n    true_order = y_true[y_true.argsort()] # y_true 크기순으로 y_true 값 정렬\n    L_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선\n    G_true = np.sum(L_mid - L_true)       # 예측이 완벽할 때 지니계수\n\n    # 정규화된 지니계수\n    return G_pred / G_true\n\n# LightGBM용 gini() 함수\ndef gini(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'gini', eval_gini(labels, preds), True # 반환값","metadata":{"execution":{"iopub.status.busy":"2023-04-03T11:01:44.686336Z","iopub.execute_input":"2023-04-03T11:01:44.686854Z","iopub.status.idle":"2023-04-03T11:01:44.698308Z","shell.execute_reply.started":"2023-04-03T11:01:44.686810Z","shell.execute_reply":"2023-04-03T11:01:44.696658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4-2. 하이퍼파라미터 최적화\n\n- 성능이 우수한 모델을 만들기 위해서 필요\n- **베이지안 최적화 기법**을 활용해 하이퍼파라미터를 조정\n\n\n\n### 4-2-1. 데이터셋 준비\n\n- 베이지안 최적화를 위한 데이터셋을 만들자","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\n# 8:2 비율로 훈련 데이터, 검증 데이터 분리 (베이지안 최적화 수행용)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size=0.2, \n                                                      random_state=0)\n\n# 베이지안 최적화용 데이터셋\nbayes_dtrain = lgb.Dataset(X_train, y_train)\nbayes_dvalid = lgb.Dataset(X_valid, y_valid)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T11:01:56.719576Z","iopub.execute_input":"2023-04-03T11:01:56.720494Z","iopub.status.idle":"2023-04-03T11:01:59.253931Z","shell.execute_reply.started":"2023-04-03T11:01:56.720440Z","shell.execute_reply":"2023-04-03T11:01:59.252714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 이렇게 나뉜 훈련 데이터와 검증 데이터를 활용해 베이지안 최적화용 데이터셋을 만들자\n\n\n### 4-2-2. 하이퍼파라미터 범위 설정\n\n1. 하이퍼파라미터 범위를 점점 좁히는 방법\n2. 다른 상위권 캐글러가 설정한 하이퍼파라미터를 참고하는 방법","metadata":{}},{"cell_type":"code","source":"# 베이지안 최적화를 위한 하이퍼파라미터 범위\nparam_bounds = {'num_leaves': (30, 40),\n                'lambda_l1': (0.7, 0.9),\n                'lambda_l2': (0.9, 1),\n                'feature_fraction': (0.6, 0.7),\n                'bagging_fraction': (0.6, 0.9),\n                'min_child_samples': (6, 10),\n                'min_child_weight': (10, 40)}\n\n# 값이 고정된 하이퍼파라미터\nfixed_params = {'objective': 'binary',\n                'learning_rate': 0.005,\n                'bagging_freq': 1,\n                'force_row_wise': True,\n                'random_state': 1991}","metadata":{"execution":{"iopub.status.busy":"2023-04-03T11:05:55.801340Z","iopub.execute_input":"2023-04-03T11:05:55.801859Z","iopub.status.idle":"2023-04-03T11:05:55.810021Z","shell.execute_reply.started":"2023-04-03T11:05:55.801812Z","shell.execute_reply":"2023-04-03T11:05:55.808629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-2-3. (베이지안 최적화용) 평가지표 계산 함수 작성","metadata":{}},{"cell_type":"code","source":"def eval_function(num_leaves, lambda_l1, lambda_l2, feature_fraction,\n                  bagging_fraction, min_child_samples, min_child_weight):\n    '''최적화하려는 평가지표(지니계수) 계산 함수'''\n    \n    # 베이지안 최적화를 수행할 하이퍼파라미터 \n    params = {'num_leaves': int(round(num_leaves)),\n              'lambda_l1': lambda_l1,\n              'lambda_l2': lambda_l2,\n              'feature_fraction': feature_fraction,\n              'bagging_fraction': bagging_fraction,\n              'min_child_samples': int(round(min_child_samples)),\n              'min_child_weight': min_child_weight,\n              'feature_pre_filter': False}\n    # 고정된 하이퍼파라미터도 추가\n    params.update(fixed_params)\n    \n    print('하이퍼파라미터:', params)    \n    \n    # LightGBM 모델 훈련\n    lgb_model = lgb.train(params=params, \n                           train_set=bayes_dtrain,\n                           num_boost_round=2500,\n                           valid_sets=bayes_dvalid,\n                           feval=gini,\n                           early_stopping_rounds=300,\n                           verbose_eval=False)\n    # 검증 데이터로 예측 수행\n    preds = lgb_model.predict(X_valid) \n    # 지니계수 계산\n    gini_score = eval_gini(y_valid, preds)\n    print(f'지니계수 : {gini_score}\\n')\n    \n    return gini_score","metadata":{"execution":{"iopub.status.busy":"2023-04-03T11:08:33.170536Z","iopub.execute_input":"2023-04-03T11:08:33.172054Z","iopub.status.idle":"2023-04-03T11:08:33.181780Z","shell.execute_reply.started":"2023-04-03T11:08:33.171991Z","shell.execute_reply":"2023-04-03T11:08:33.180420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-2-4. 최적화 수행","metadata":{}},{"cell_type":"code","source":"from bayes_opt import BayesianOptimization\n\n# 베이지안 최적화 객체 생성\noptimizer = BayesianOptimization(f=eval_function,      # 평가지표 계산 함수\n                                 pbounds=param_bounds, # 하이퍼파라미터 범위\n                                 random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T11:09:33.178806Z","iopub.execute_input":"2023-04-03T11:09:33.179281Z","iopub.status.idle":"2023-04-03T11:09:33.228469Z","shell.execute_reply.started":"2023-04-03T11:09:33.179243Z","shell.execute_reply":"2023-04-03T11:09:33.227570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 베이지안 최적화 수행\noptimizer.maximize(init_points=3, n_iter=6)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T11:09:38.223115Z","iopub.execute_input":"2023-04-03T11:09:38.223529Z","iopub.status.idle":"2023-04-03T11:41:58.227120Z","shell.execute_reply.started":"2023-04-03T11:09:38.223494Z","shell.execute_reply":"2023-04-03T11:41:58.225719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-2-5. 결과 확인","metadata":{}},{"cell_type":"code","source":"# 평가함수 점수가 최대일 때 하이퍼파라미터\nmax_params = optimizer.max['params']\nmax_params","metadata":{"execution":{"iopub.status.busy":"2023-04-03T11:41:58.229634Z","iopub.execute_input":"2023-04-03T11:41:58.230978Z","iopub.status.idle":"2023-04-03T11:41:58.239945Z","shell.execute_reply.started":"2023-04-03T11:41:58.230926Z","shell.execute_reply":"2023-04-03T11:41:58.238730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 정수형 하이퍼파라미터 변환\nmax_params['num_leaves'] = int(round(max_params['num_leaves']))\nmax_params['min_child_samples'] = int(round(max_params['min_child_samples']))","metadata":{"execution":{"iopub.status.busy":"2023-04-03T11:41:58.241556Z","iopub.execute_input":"2023-04-03T11:41:58.242103Z","iopub.status.idle":"2023-04-03T11:41:58.251078Z","shell.execute_reply.started":"2023-04-03T11:41:58.242051Z","shell.execute_reply":"2023-04-03T11:41:58.249717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 값이 고정된 하이퍼파라미터 추가\nmax_params.update(fixed_params)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T11:41:58.254390Z","iopub.execute_input":"2023-04-03T11:41:58.254950Z","iopub.status.idle":"2023-04-03T11:41:58.266976Z","shell.execute_reply.started":"2023-04-03T11:41:58.254897Z","shell.execute_reply":"2023-04-03T11:41:58.265660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_params","metadata":{"execution":{"iopub.status.busy":"2023-04-03T11:41:58.269102Z","iopub.execute_input":"2023-04-03T11:41:58.270048Z","iopub.status.idle":"2023-04-03T11:41:58.281299Z","shell.execute_reply.started":"2023-04-03T11:41:58.269999Z","shell.execute_reply":"2023-04-03T11:41:58.280054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4-3. 모델 훈련 및 성능 검증\n\n- 베이지안 최적화로 찾은 하이퍼파라미터를 활용해 `LightGBM` 모델을 다시 훈련해보자","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\n# 층화 K 폴드 교차 검증기 생성\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)\n\n# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\noof_val_preds = np.zeros(X.shape[0]) \n# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\noof_test_preds = np.zeros(X_test.shape[0]) \n\n# OOF 방식으로 모델 훈련, 검증, 예측\nfor idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n    # 각 폴드를 구분하는 문구 출력\n    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n    \n    # 훈련용 데이터, 검증용 데이터 설정\n    X_train, y_train = X[train_idx], y[train_idx] # 훈련용 데이터\n    X_valid, y_valid = X[valid_idx], y[valid_idx] # 검증용 데이터\n\n    # LightGBM 전용 데이터셋 생성\n    dtrain = lgb.Dataset(X_train, y_train) # LightGBM 전용 훈련 데이터셋\n    dvalid = lgb.Dataset(X_valid, y_valid) # LightGBM 전용 검증 데이터셋\n                          \n    # LightGBM 모델 훈련\n    lgb_model = lgb.train(params=max_params,    # 최적 하이퍼파라미터\n                          train_set=dtrain,     # 훈련 데이터셋\n                          num_boost_round=2500, # 부스팅 반복 횟수\n                          valid_sets=dvalid,    # 성능 평가용 검증 데이터셋\n                          feval=gini,           # 검증용 평가지표\n                          early_stopping_rounds=300, # 조기종료 조건\n                          verbose_eval=100)     # 100번째마다 점수 출력\n    \n    # 테스트 데이터를 활용해 OOF 예측\n    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측 \n    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n    \n    # 검증 데이터 예측확률에 대한 정규화 지니계수\n    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n    print(f'폴드 {idx+1} 지니계수 : {gini_score}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T11:41:58.283408Z","iopub.execute_input":"2023-04-03T11:41:58.284309Z","iopub.status.idle":"2023-04-03T12:06:17.979935Z","shell.execute_reply.started":"2023-04-03T11:41:58.284260Z","shell.execute_reply":"2023-04-03T12:06:17.978578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('OOF 검증 데이터 지니계수 :', eval_gini(y, oof_val_preds))","metadata":{"execution":{"iopub.status.busy":"2023-04-03T12:06:17.982014Z","iopub.execute_input":"2023-04-03T12:06:17.982437Z","iopub.status.idle":"2023-04-03T12:06:18.123812Z","shell.execute_reply.started":"2023-04-03T12:06:17.982396Z","shell.execute_reply":"2023-04-03T12:06:18.122457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4-4. 예측 및 결과 제출","metadata":{}},{"cell_type":"code","source":"submission['target'] = oof_test_preds\nsubmission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T12:06:18.125820Z","iopub.execute_input":"2023-04-03T12:06:18.126254Z","iopub.status.idle":"2023-04-03T12:06:20.501901Z","shell.execute_reply.started":"2023-04-03T12:06:18.126213Z","shell.execute_reply":"2023-04-03T12:06:20.500625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}